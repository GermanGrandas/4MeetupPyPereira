{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento \n",
    "\n",
    "## Paso 1: Limpiando el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cadena sin Link: When I went to New York,the parking is way too expensive!!,you'll find one for --$,  -new- l You can go to this adress to fing good deals. Leave you luggage and go park your cars!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cadena = \"When I went to New York,the parking is way too expensive!!,you'll find one for --$, http:centralparking.com-new-yorkwthst.html You can go to this adress to fing good deals. Leave you luggage and go park your cars!\"\n",
    "\n",
    "cadenaLimpia = re.sub('(http(s?):(//)?)?(\\w+\\.)?(\\w+\\.\\w{3}(/.+)?)',' ',cadena)\n",
    "print(\"Cadena sin Link: {}\".format(cadenaLimpia))\n",
    "\n",
    "#cadenaLimpia = re.sub('[^a-zA-Z]',' ',cadenaSinLinks)\n",
    "#print(\"Cadena sin otros carecteres: {}\".format(cadenaLimpia))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Tokenización, Lemanización o Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'went', 'to', 'New', 'York', 'the', 'parking', 'is', 'way', 'too', 'expensive', 'you', 'll', 'find', 'one', 'for', 'new', 'l', 'You', 'can', 'go', 'to', 'this', 'adress', 'to', 'fing', 'good', 'deals', 'Leave', 'you', 'luggage', 'and', 'go', 'park', 'your', 'cars']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words =tokenizer.tokenize(cadenaLimpia)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'went', 'to', 'New', 'York', ',', 'the', 'parking', 'is', 'way', 'too', 'expensive', '!', '!', ',', 'you', \"'ll\", 'find', 'one', 'for', '--', '$', ',', '-new-', 'l', 'You', 'can', 'go', 'to', 'this', 'adress', 'to', 'fing', 'good', 'deals', '.', 'Leave', 'you', 'luggage', 'and', 'go', 'park', 'your', 'cars', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(cadenaLimpia)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpiando StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'now', 'at', 'and', 'when', 'needn', 'having', 'all', \"she's\", 'the', 'more', 'be', 'being', 'very', 'same', 'few', \"you'll\", \"needn't\", 'above', 'this', 'yourselves', 'won', \"won't\", \"hadn't\", 'because', 'isn', \"haven't\", 'down', 'just', 'again', 'we', 'doesn', 'below', 'under', 'my', 'shouldn', 'have', 'against', 'both', 'they', 'hadn', \"shouldn't\", 'then', 'each', 'couldn', 'before', \"hasn't\", 'as', 'wouldn', 're', 'yours', 'aren', 'will', 'no', \"aren't\", \"you'd\", 'his', 'that', 'out', \"doesn't\", 'hers', 'after', 'don', 'any', 'has', \"wasn't\", \"wouldn't\", 'it', 'further', 'off', 'in', 'where', 'o', 'so', 'i', 'are', 'themselves', \"mustn't\", \"mightn't\", 'her', \"didn't\", 'between', 'ourselves', 'do', 'those', \"shan't\", 'himself', \"isn't\", 'during', 'of', 'mightn', 'didn', \"it's\", 'was', 's', 'some', 'them', 'ours', 'll', 'am', 'what', 'myself', 'you', 'who', 'ain', 'mustn', 'herself', 'yourself', 'had', 'how', 'nor', 'through', 'but', \"couldn't\", 'can', 'own', 'their', 'or', 'to', 'why', 'were', 'once', 'until', 'on', 've', 'our', 'other', 'these', 'such', 'd', 'by', \"you're\", 'y', \"that'll\", 'if', 'wasn', 'with', 'me', 'here', 'been', 'up', \"weren't\", \"don't\", 'there', 'not', 'should', 'most', \"should've\", 'm', 'from', 'he', 'did', 'weren', 'which', 'into', 'its', 'only', 'is', 'ma', 'him', 'she', 'than', 'does', 'doing', 't', 'an', 'while', 'itself', 'haven', 'your', 'a', 'about', 'shan', 'over', 'hasn', 'too', \"you've\", 'whom', 'theirs'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'went', 'New', 'York', 'parking', 'way', 'expensive', 'find', 'one', 'new', 'l', 'You', 'go', 'adress', 'fing', 'good', 'deals', 'Leave', 'luggage', 'go', 'park', 'cars']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenCorpus = [word for word in words if not word in stopWords]\n",
    "print(tokenCorpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemanización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'go', 'New', 'York', 'park', 'way', 'expensive', 'find', 'one', 'new', 'l', 'You', 'go', 'adress', 'fing', 'good', 'deal', 'Leave', 'luggage', 'go', 'park', 'cars']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem  import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "corpusLemma = []\n",
    "for x in tokenCorpus:\n",
    "    corpusLemma.append(wordnet_lemmatizer.lemmatize(x,pos='v'))\n",
    "print(corpusLemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'i', 'go', 'new', 'york', 'park', 'way', 'expend', 'find', 'on', 'new', 'l', 'you', 'go', 'adress', 'fing', 'good', 'deal', 'leav', 'lug', 'go', 'park', 'car']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "LS = LancasterStemmer()\n",
    "corpus = []\n",
    "for x in corpusLemma:\n",
    "    corpus.append(LS.stem(x))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando la bolsa de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adress', 'car', 'deal', 'expend', 'find', 'fing', 'go', 'good', 'leav', 'lug', 'new', 'on', 'park', 'way', 'when', 'york', 'you']\n",
      "----------------------\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "w = cv.get_feature_names()\n",
    "\n",
    "print(w)\n",
    "print(\"----------------------\")\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando un modelo para Análisis de Sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "\n",
    "INITIAL = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesando los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t',quoting=3)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getCorpus(data):\n",
    "    ps = PorterStemmer()\n",
    "    corpus = []\n",
    "    for review in data:\n",
    "        x = re.sub('[^a-zA-Z]',' ',review).lower().split()\n",
    "        corpus.append(' '.join([ps.stem(word) for word in x if not word in set(stopwords.words('english'))]))\n",
    "    return corpus\n",
    "\n",
    "def getWordBag(data):    \n",
    "    corpus = getCorpus(data)\n",
    "    maxF = 1500\n",
    "    cv = CountVectorizer(max_features=maxF)\n",
    "    X = cv.fit_transform(corpus).toarray()\n",
    "    \n",
    "    global INITIAL\n",
    "    if np.all(INITIAL ==None):\n",
    "         INITIAL = np.array(corpus)\n",
    "    if X.shape[1] < maxF:\n",
    "        y=cv.fit_transform(INITIAL).toarray()\n",
    "        aux  = [[0]*y.shape[1]]\n",
    "        w = cv.get_feature_names()\n",
    "        for word in corpus:\n",
    "            for y in word.split():\n",
    "                if y in w:\n",
    "                    pos = w.index(y)\n",
    "                    aux[0][pos] = 1\n",
    "        X = np.array(aux)\n",
    "    return X\n",
    "\n",
    "def liked(review):\n",
    "    if review == [False]:\n",
    "        return \"Comentario Negativo.\"\n",
    "    else:\n",
    "        return \"Comentario Positivo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = getWordBag(dataset['Review'])\n",
    "y = dataset['Liked']\n",
    "X_train , X_test, y_train, y_test = train_test_split(X,y,test_size= 0.25,random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando una red Neuronal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1500)\n",
      "Epoch 1/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.6814 - acc: 0.5827\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.5644 - acc: 0.7480\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.3907 - acc: 0.8320\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.2931 - acc: 0.8773\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.2421 - acc: 0.9013\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.1896 - acc: 0.9200\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.1494 - acc: 0.9360\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.1412 - acc: 0.9507\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1009 - acc: 0.9560A: 1s - loss: 0\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.0744 - acc: 0.9720\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.0732 - acc: 0.9613\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.0540 - acc: 0.9813\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.0507 - acc: 0.9787A: 0s - loss: 0.0432 - acc: 0.\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.0562 - acc: 0.9813\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.0576 - acc: 0.9813\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0489 - acc: 0.9840\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0376 - acc: 0.9853\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.0308 - acc: 0.9893\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.0302 - acc: 0.9907\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0343 - acc: 0.9933\n",
      "el tiempo fue: 1.1721448858579 minutos\n"
     ]
    }
   ],
   "source": [
    "Ann = Sequential()\n",
    "shape = int(X.shape[1]/2)\n",
    "print(X.shape)\n",
    "Ann.add(Dense(units=shape,kernel_initializer='uniform',activation='relu',input_dim=X.shape[1]))\n",
    "Ann.add(Dropout(0.7))\n",
    "Ann.add(Dense(units=shape,kernel_initializer='uniform',activation='relu'))\n",
    "Ann.add(Dropout(0.7))\n",
    "Ann.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n",
    "        \n",
    "Ann.compile(optimizer='RMSprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "from time import time\n",
    "        \n",
    "t0 = time()\n",
    "Ann.fit(X_train,y_train,batch_size=10,epochs=20)\n",
    "t1 = time()\n",
    "print(\"el tiempo fue: {} minutos\".format((t1-t0)/60))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicciones y resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[92 25]\n",
      " [34 99]]\n",
      "76.4\n"
     ]
    }
   ],
   "source": [
    "y_pred = Ann.predict(X_test)\n",
    "y_pred = (y_pred>0.5)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "accuracy = ((cm[0][0]+cm[1][1])/X_test.shape[0])*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haciendo una prediccion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario Positivo.\n",
      "Comentario Negativo.\n"
     ]
    }
   ],
   "source": [
    "pred = \"Service was very prompt\"\n",
    "#pred = \"Crust is not good\"\n",
    "X1 = getWordBag([pred])\n",
    "y1 = Ann.predict(X1) > 0.5\n",
    "print(liked(y1))\n",
    "\n",
    "\n",
    "pred = \"Honeslty it didn't taste THAT fresh.)\"\n",
    "X1 = getWordBag([pred])\n",
    "y1 = Ann.predict(X1) > 0.5\n",
    "print(liked(y1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Clasificador bayesiano Ingenuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 67  50]\n",
      " [ 20 113]]\n",
      "72.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "   \n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "accuracy = ((cm[0][0]+cm[1][1])/X_test.shape[0])*100\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario Positivo.\n",
      "Comentario Negativo.\n"
     ]
    }
   ],
   "source": [
    "pred = \"Service was very prompt\"\n",
    "X1 = getWordBag([pred])\n",
    "y1 = classifier.predict(X1) > 0.5\n",
    "print(liked(y1))\n",
    "\n",
    "pred = \"Honeslty it didn't taste THAT fresh.)\"\n",
    "X1 = getWordBag([pred])\n",
    "y1 = classifier.predict(X1) > 0.5\n",
    "print(liked(y1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
